\title{Assignment One: Neural Networks}
\author{
        Stephen Arnold \\
        Department of Computer Science\\
        George Mason University\\
            \and
        David Freelan\\
        Department of Computer Science\\
        George Mason University\\
}
\date{\today}

\documentclass[11pt,letterpaper]{article}
%\usepackage[letterpaper,margin=0.75in]{geometry}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
    frame=single,
		breaklines=true,
	  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}

%\graphicspath{ {./analysis/graphs/} }
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

%\setlength\parindent{0pt} % Removes all indentation from paragraphs

\begin{document}
\maketitle

\section{Introduction}
An artificial neural network (ANN, or NN) is a statistical learning algorithm based (loosely) on the characteristics of the human brain.
The NN accepts a fixed number of inputs and calculates a fixed number of numeric outputs.
A typical feature of a NN is a layer of unobservable, or hidden, units. 
These units are called nodes or neurons.
Input nodes are connected to hidden or output nodes by weight values which modify the value of each subsequent node.

This project report details our experiments in changing various Neural Network parameters and
the effects those modifications had upon the ability of the Neural Network to converge to a solution.
Our neural network assignment went a little beyond the default assignment.
We really wanted to know: what really happens when you change alpha, and when you change the number of neurons?
As a result, we ended up re-discovering a method of training, the use of a validation set.

\subsection{Motives}
One of the topics discussed in class was overfitting and under fitting.
We want to visualize this as its happening on a real dataset.
We have two variables we're interested in: number of neurons, and the value of alpha.
Alpha is the learning rate of the neural net, and the neuron width is the number of neurons in the hidden layer of the network.
Both of these settings should have some effect on the fitting of the dataset.

\section{Procedure}
Our Neural Network made use of the simple-generalization method for classification of the data sets.
To train and test our classifier, the provided dataset was divided in two: half of the data for training, and half for testing.
A single network was iterated through 10,000 times, and 100 networks were realized for statistical analysis.
After training the neural network, the testing data was applied and the realized errors were accumulated over an entire run.
All tests were done on the supplied Voting Records training set using the the Simple Generalization method discussed in the homework. 

\section{Results}
The accumulated error for three different hidden node widths may be seen in Figure \ref{fig:Neurons}.
These three neural networks - with 2, 6 and 13 neurons - were run using a constant $\alpha = 0.1$. 
In Figure \ref{fig:Alphas}, the effect that different learning rates (\alpha) has on the accumulated error rates can be seen.
For a Hidden Layer of 6 neurons, $\alpha = 0.05$ shows a greater error before dropping off during later trials.
This is consistent with ones intuition that one would expect to have more errors from a slow learner.
However, for $\alpha = 0.5$, the NN learns a little too quickly and begins to overfit the data during later trials of the input data.

\begin{figure}
\begin{center}
\includegraphics[width=4.8in]{{Classification-alpha0.1Neurons2-6-13}.png}
\caption{\small \sl Accumulated error across training/testing runs. More hidden nodes relates to less error ... quicker.\label{fig:Neurons}} 
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=4.8in]{{Classification_6_neurons_alpha0.05-0.1-0.5}.png}
\caption{\small \sl Learning rate (alpha) has an effect on error, but learning too fast can lead to overfitting the data.\label{fig:Alphas}} 
\end{center}
\end{figure}

\section{Conclusion}
An interesting observation from the results of these experiments was the behavior of overfitting.
There appears to be a period where each graph achieves a minimal error (usually within the first 1000 iterations).
Following this initial minimum, the observed error becomes progressively worse.
Given this insight, it is believed that the use of a validation set could verify a successful training had been achieved.
Training simply train until your validation set starts to overfit, and immediately stop training. However, while we tested with a wide range of alphas and hidden neurons, we only tested it with one problem set. In the future to get a better idea of when to stop training, we would try a larger subset of problems and see if the same result happens.



%\fbox{\includegraphics[page=3,scale=0.4]{./experiments/AlphaNeurons.pdf}}
%\includegraphics[scale=0.4]{{alpha0.1Neurons1-12}.png}

\newpage
\lstinputlisting[language=Lisp]{../nn-sean.lisp}

\end{document} 

(alpha - the learning rate - and the number of hidden nodes) 
