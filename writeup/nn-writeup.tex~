\title{Assignment One: Neural Networks}
\author{
        Stephen Arnold \\
        Department of Computer Science\\
        George Mason University\\
            \and
        David Freelan\\
        Department of Computer Science\\
        George Mason University\\
}
\date{\today}

\documentclass[11pt,letterpaper]{article}
%\usepackage[letterpaper,margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
    frame=single,
		breaklines=true,
	  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}

%\graphicspath{ {./analysis/graphs/} }
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

%\setlength\parindent{0pt} % Removes all indentation from paragraphs

\begin{document}
\maketitle

\section{Introduction}
This project report details our experiments in changing various Neural Network parameters and
the effects those modifications had upon the ability of the Neural Network to converge to a solution.
Our neural network assignment went a little beyond the default assignment.
We really wanted to know: what really happens when you change alpha, and when you change the number of neurons?
As a result, we ended up re-discovering a method of training, the use of a validation set.

\subsection{Motives}
One of the topics discussed in class was overfitting and under fitting.
We want to visualize this as its happening on a real dataset.
We have two variables we're interested in: number of neurons, and the value of alpha.
Alpha is the learning rate of the neural net, and the neuron width is the number of neurons in the hidden layer of the network.
Both of these settings should have some effect on the fitting of the dataset.

\section{Procedure}
Our Neural Network made use of the simple-generalization method for classification of the data sets.
To train and test our classifier, the provided dataset was divided in two: half of the data for training, and half for testing.
A single network was iterated through 10,000 times, and 100 networks were realized for statistical analysis.
After training the neural network, the testing data was applied and the realized errors were accumulated over an entire run.
All tests were done on the supplied Voting Records training set using the the Simple Generalization method discussed in the homework. 

\section{Results}
The accumulated error for three different hidden node widths may be seen in Figure \ref{fig:Neurons}.
These three neural networks - with 2, 6 and 13 neurons - were run using a constant alpha = 0.1. 


\begin{figure}
\begin{center}
\includegraphics[width=4.8in]{{Classification-alpha0.1Neurons2-6-13}.png}
\caption{\small \sl This figure shows results of Nobel prize winning importance.\label{fig:Neurons}} 
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=4.8in]{{Classification_6_neurons_alpha0.05-0.1-0.5}.png}
\caption{\small \sl This figure shows results of more Nobel prize winning importance.\label{fig:Alphas}} 
\end{center}
\end{figure}

\section{Conclusion}
An interesting observation from the results of these experiments was the behavior of overfitting.
There appears to be a period where each graph achieves a minimal error (usually within the first 1000 iterations).
Following this initial minimum, the observed error becomes progressively worse.
Given this insight, it is believed that the use of a validation set could verify a successful training had been achieved.
Training simply train until your validation set starts to overfit, and immediately stop training. However, while we tested with a wide range of alphas and hidden neurons, we only tested it with one problem set. In the future to get a better idea of when to stop training, we would try a larger subset of problems and see if the same result happens.



%\fbox{\includegraphics[page=3,scale=0.4]{./experiments/AlphaNeurons.pdf}}
%\includegraphics[scale=0.4]{{alpha0.1Neurons1-12}.png}

\newpage
\lstinputlisting[language=Lisp]{../nn-sean.lisp}

\end{document} 

(alpha - the learning rate - and the number of hidden nodes) 
